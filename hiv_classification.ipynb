{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d3cc6",
   "metadata": {},
   "source": [
    "# HIV Activity Classification\n",
    "This is the main HW notebook for the HIV classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "071df8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.utils import scatter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2ce86",
   "metadata": {},
   "source": [
    "# TODO: ADD DEPENDENCY INSTALLATION FOR COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb88782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>HIV_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=S(=O)(O)CCS(=O)(=O)O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41122</th>\n",
       "      <td>CCC1CCC2c3c([nH]c4ccc(C)cc34)C3C(=O)N(N(C)C)C(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41123</th>\n",
       "      <td>Cc1ccc2[nH]c3c(c2c1)C1CCC(C(C)(C)C)CC1C1C(=O)N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41124</th>\n",
       "      <td>Cc1ccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41125</th>\n",
       "      <td>Cc1cccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41126</th>\n",
       "      <td>CCCCCC=C(c1cc(Cl)c(OC)c(-c2nc(C)no2)c1)c1cc(Cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41127 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  smiles  HIV_active\n",
       "0      CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...           0\n",
       "1      C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...           0\n",
       "2                       CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21           0\n",
       "3        Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1           0\n",
       "4                                 O=S(=O)(O)CCS(=O)(=O)O           0\n",
       "...                                                  ...         ...\n",
       "41122  CCC1CCC2c3c([nH]c4ccc(C)cc34)C3C(=O)N(N(C)C)C(...           0\n",
       "41123  Cc1ccc2[nH]c3c(c2c1)C1CCC(C(C)(C)C)CC1C1C(=O)N...           0\n",
       "41124  Cc1ccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)C...           0\n",
       "41125  Cc1cccc(N2C(=O)C3c4[nH]c5ccccc5c4C4CCC(C(C)(C)...           0\n",
       "41126  CCCCCC=C(c1cc(Cl)c(OC)c(-c2nc(C)no2)c1)c1cc(Cl...           0\n",
       "\n",
       "[41127 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#root_folder = colab_root_folder = os.getcwd() # for when we get on colab\n",
    "\n",
    "path = \"hiv.csv\"\n",
    "data_pd = pd.read_csv(path)\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c977d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41127, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_arr = np.array(data_pd)\n",
    "data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78c0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    \"\"\"\n",
    "    Maps input elements x which are not in the permitted list to the last element\n",
    "    of the permitted list.\n",
    "    \"\"\"\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]\n",
    "    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n",
    "    return binary_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d2e58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, \n",
    "                      use_chirality = True, \n",
    "                      hydrogens_implicit = True):\n",
    "    \"\"\"\n",
    "    Takes an RDKit atom object as input and gives a 1d-numpy array of atom features as output.\n",
    "    \"\"\"\n",
    "\n",
    "    # define list of permitted atoms\n",
    "    \n",
    "    permitted_list_of_atoms =  ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                                'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag',\n",
    "                                'Pd','Co','Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd',\n",
    "                                'In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']\n",
    "    \n",
    "    if hydrogens_implicit == False:\n",
    "        permitted_list_of_atoms = ['H'] + permitted_list_of_atoms\n",
    "    \n",
    "    # compute atom features\n",
    "    \n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_list_of_atoms)\n",
    "    \n",
    "    n_heavy_neighbors_enc = one_hot_encoding(int(atom.GetDegree()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, \"Extreme\"])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
    "    \n",
    "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
    "    \n",
    "    atomic_mass_scaled = [float((atom.GetMass() - 10.812)/116.092)]\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)]\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)]\n",
    "\n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "                                    \n",
    "    if use_chirality == True:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "    \n",
    "    if hydrogens_implicit == True:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "\n",
    "    return np.array(atom_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88a64b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, \n",
    "                      use_stereochemistry = True):\n",
    "    \"\"\"\n",
    "    Takes an RDKit bond object as input and gives a 1d-numpy array of bond features as output.\n",
    "    \"\"\"\n",
    "\n",
    "    permitted_list_of_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "\n",
    "    bond_type_enc = one_hot_encoding(bond.GetBondType(), permitted_list_of_bond_types)\n",
    "    \n",
    "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry == True:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "\n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd657aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pytorch_geom(x_smiles, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "    x_smiles = [smiles_1, smiles_2, ....] ... a list of SMILES strings\n",
    "    y = [y_1, y_2, ...] ... a list of numerial labels for the SMILES strings (such as associated pKi values)\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    data_list = [G_1, G_2, ...] ... a list of torch_geometric.data.Data objects which represent labeled molecular graphs that can readily be used for machine learning\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        \n",
    "        \n",
    "        # convert SMILES to RDKit mol object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds()\n",
    "        # creating of O2 is just to get the number n_node/edge_features for creating tensors.\n",
    "        # Barzilay might have more elegant code, but this is good enough.\n",
    "        unrelated_smiles = \"O=O\"\n",
    "        unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n",
    "        n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n",
    "        n_edge_features = len(get_bond_features(unrelated_mol.GetBondBetweenAtoms(0,1)))\n",
    "\n",
    "        # construct node feature matrix X of shape (n_nodes, n_node_features)\n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "            \n",
    "        X = torch.tensor(X, dtype = torch.float)\n",
    "        \n",
    "        # construct edge index array E of shape (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol)) # atoms that are adjacent\n",
    "        torch_rows = torch.from_numpy(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.from_numpy(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim = 0)\n",
    "        \n",
    "        # construct edge feature array EF of shape (n_edges, n_edge_features)\n",
    "        EF = np.zeros((n_edges, n_edge_features))\n",
    "        \n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            \n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        \n",
    "        EF = torch.tensor(EF, dtype = torch.float)\n",
    "        \n",
    "        # construct label tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype = torch.float)\n",
    "        \n",
    "        # construct Pytorch Geometric data object and append to data list\n",
    "        # NOTE, if only want to incorporate node info, can turn off edge_attr\n",
    "        data_list.append(Data(x = X, edge_index = E, edge_attr = EF, y = y_tensor))\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3807de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...[22:49:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:49:02] WARNING: not removing hydrogen atom without neighbors\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "dataset = create_pytorch_geom(data_arr[:, 0], data_arr[:, 1])\n",
    "#TODO, need to make dataset labels a 1-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "640b7ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 tensor([1.])\n",
      "16 tensor([1.])\n",
      "80 tensor([1.])\n",
      "203 tensor([1.])\n",
      "234 tensor([1.])\n",
      "235 tensor([1.])\n",
      "244 tensor([1.])\n",
      "271 tensor([1.])\n",
      "279 tensor([1.])\n",
      "326 tensor([1.])\n",
      "352 tensor([1.])\n",
      "353 tensor([1.])\n",
      "361 tensor([1.])\n",
      "384 tensor([1.])\n",
      "387 tensor([1.])\n",
      "429 tensor([1.])\n",
      "434 tensor([1.])\n",
      "443 tensor([1.])\n",
      "498 tensor([1.])\n",
      "499 tensor([1.])\n",
      "654 tensor([1.])\n",
      "676 tensor([1.])\n",
      "699 tensor([1.])\n",
      "740 tensor([1.])\n",
      "818 tensor([1.])\n",
      "869 tensor([1.])\n",
      "879 tensor([1.])\n",
      "978 tensor([1.])\n",
      "996 tensor([1.])\n",
      "1004 tensor([1.])\n",
      "1059 tensor([1.])\n",
      "1153 tensor([1.])\n",
      "1195 tensor([1.])\n",
      "1196 tensor([1.])\n",
      "1198 tensor([1.])\n",
      "1199 tensor([1.])\n",
      "1200 tensor([1.])\n",
      "1212 tensor([1.])\n",
      "1266 tensor([1.])\n",
      "1277 tensor([1.])\n",
      "1322 tensor([1.])\n",
      "1461 tensor([1.])\n",
      "1482 tensor([1.])\n",
      "1510 tensor([1.])\n",
      "1511 tensor([1.])\n",
      "1528 tensor([1.])\n",
      "1530 tensor([1.])\n",
      "1532 tensor([1.])\n",
      "1533 tensor([1.])\n",
      "1534 tensor([1.])\n",
      "1535 tensor([1.])\n",
      "1571 tensor([1.])\n",
      "1572 tensor([1.])\n",
      "1575 tensor([1.])\n",
      "1576 tensor([1.])\n",
      "1577 tensor([1.])\n",
      "1578 tensor([1.])\n",
      "1579 tensor([1.])\n",
      "1580 tensor([1.])\n",
      "1583 tensor([1.])\n",
      "1584 tensor([1.])\n",
      "1585 tensor([1.])\n",
      "1586 tensor([1.])\n",
      "1587 tensor([1.])\n",
      "1588 tensor([1.])\n",
      "1589 tensor([1.])\n",
      "1630 tensor([1.])\n",
      "1637 tensor([1.])\n",
      "1638 tensor([1.])\n",
      "1639 tensor([1.])\n",
      "1641 tensor([1.])\n",
      "1643 tensor([1.])\n",
      "1644 tensor([1.])\n",
      "1729 tensor([1.])\n",
      "1825 tensor([1.])\n",
      "1826 tensor([1.])\n",
      "1909 tensor([1.])\n",
      "1915 tensor([1.])\n",
      "1966 tensor([1.])\n",
      "1976 tensor([1.])\n",
      "1977 tensor([1.])\n",
      "1978 tensor([1.])\n",
      "2016 tensor([1.])\n",
      "2025 tensor([1.])\n",
      "2069 tensor([1.])\n",
      "2158 tensor([1.])\n",
      "2186 tensor([1.])\n",
      "2187 tensor([1.])\n",
      "2188 tensor([1.])\n",
      "2190 tensor([1.])\n",
      "2191 tensor([1.])\n",
      "2192 tensor([1.])\n",
      "2210 tensor([1.])\n",
      "2211 tensor([1.])\n",
      "2212 tensor([1.])\n",
      "2213 tensor([1.])\n",
      "2279 tensor([1.])\n",
      "2301 tensor([1.])\n",
      "2302 tensor([1.])\n",
      "2303 tensor([1.])\n",
      "2305 tensor([1.])\n",
      "2308 tensor([1.])\n",
      "2314 tensor([1.])\n",
      "2322 tensor([1.])\n",
      "2423 tensor([1.])\n",
      "2426 tensor([1.])\n",
      "2510 tensor([1.])\n",
      "2511 tensor([1.])\n",
      "2529 tensor([1.])\n",
      "2537 tensor([1.])\n",
      "2580 tensor([1.])\n",
      "2599 tensor([1.])\n",
      "2606 tensor([1.])\n",
      "2612 tensor([1.])\n",
      "2616 tensor([1.])\n",
      "2641 tensor([1.])\n",
      "2651 tensor([1.])\n",
      "2775 tensor([1.])\n",
      "2787 tensor([1.])\n",
      "2805 tensor([1.])\n",
      "2867 tensor([1.])\n",
      "2868 tensor([1.])\n",
      "2899 tensor([1.])\n",
      "2900 tensor([1.])\n",
      "2901 tensor([1.])\n",
      "2902 tensor([1.])\n",
      "2903 tensor([1.])\n",
      "2904 tensor([1.])\n",
      "2905 tensor([1.])\n",
      "2906 tensor([1.])\n",
      "2907 tensor([1.])\n",
      "2908 tensor([1.])\n",
      "2923 tensor([1.])\n",
      "2925 tensor([1.])\n",
      "2949 tensor([1.])\n",
      "2955 tensor([1.])\n",
      "2956 tensor([1.])\n",
      "2964 tensor([1.])\n",
      "2965 tensor([1.])\n",
      "2967 tensor([1.])\n",
      "2968 tensor([1.])\n",
      "2969 tensor([1.])\n",
      "2970 tensor([1.])\n",
      "2971 tensor([1.])\n",
      "2972 tensor([1.])\n",
      "2973 tensor([1.])\n",
      "2975 tensor([1.])\n",
      "2999 tensor([1.])\n",
      "3081 tensor([1.])\n",
      "3156 tensor([1.])\n",
      "3157 tensor([1.])\n",
      "3190 tensor([1.])\n",
      "3239 tensor([1.])\n",
      "3267 tensor([1.])\n",
      "3291 tensor([1.])\n",
      "3299 tensor([1.])\n",
      "3310 tensor([1.])\n",
      "3371 tensor([1.])\n",
      "3394 tensor([1.])\n",
      "3502 tensor([1.])\n",
      "3503 tensor([1.])\n",
      "3535 tensor([1.])\n",
      "3540 tensor([1.])\n",
      "3813 tensor([1.])\n",
      "3894 tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, d in enumerate(dataset):\n",
    "    if d.y == 1:\n",
    "        print (i, d.y)\n",
    "    count += 1\n",
    "    if count == 4000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3708083b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41127"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].y\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb41c0",
   "metadata": {},
   "source": [
    "# Graph Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b61da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "# the kipf and welling GCN\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # these could be He initialized. \n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \n",
    "        # TODO: make blanks out of this one.\n",
    "        # adj in batch would be a block diagonal matrix??? probably, not...Dataloader takes care of that.\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    # Implementation from troch.geometric\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        \n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        # Step 6: Apply a final bias vector.\n",
    "        out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "        \n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a2da259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_add_pool(x: torch.Tensor, batch: [torch.Tensor],\n",
    "                    size: [int] = None) -> torch.Tensor:\n",
    "    r\"\"\"Returns batch-wise graph-level-outputs by adding node features\n",
    "    across the node dimension, so that for a single graph\n",
    "    :math:`\\mathcal{G}_i` its output is computed by\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{r}_i = \\sum_{n=1}^{N_i} \\mathbf{x}_n.\n",
    "\n",
    "    Functional method of the\n",
    "    :class:`~torch_geometric.nn.aggr.SumAggregation` module.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Node feature matrix\n",
    "            :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "        batch (torch.Tensor, optional): The batch vector\n",
    "            :math:`\\mathbf{b} \\in {\\{ 0, \\ldots, B-1\\}}^N`, which assigns\n",
    "            each node to a specific example.\n",
    "        size (int, optional): The number of examples :math:`B`.\n",
    "            Automatically calculated if not given. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    dim = -1 if x.dim() == 1 else -2\n",
    "\n",
    "    if batch is None:\n",
    "        return x.sum(dim=dim, keepdim=x.dim() <= 2)\n",
    "    size = int(batch.max().item() + 1) if size is None else size\n",
    "    return scatter(x, batch, dim=dim, dim_size=size, reduce='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bdb7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugGCN(Module):\n",
    "    def __init__(self, in_features, hidden_features=[], out_features=1, \n",
    "                 activation=nn.ReLU(), pool=global_add_pool):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.activation=activation\n",
    "        \n",
    "        gc1 = GraphConvolution(self.in_features, self.hidden_features[0])\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for i, h in enumerate(self.hidden_features[1:]):\n",
    "            self.hidden_layers.append(GraphConvolution(self.hidden_features[i - 1], h))\n",
    "            \n",
    "        \n",
    "        self.pool=global_add_pool\n",
    "        \n",
    "        self.out_layer = nn.Linear(self.hidden_features[-1], out_features)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_graph, edge_index):\n",
    "        # get adjacency matrix\n",
    "        # assume precomputed adjacency matrix.\n",
    "        # implementation based off of kipf and welling\n",
    "        adj, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        x = self.gc1(input_graph, adj)\n",
    "        \n",
    "        for l in self.hidden_layers:\n",
    "            x = self.activation(l(x, adj))\n",
    "        \n",
    "        self.pool(x)\n",
    "        x = self.out_layer(x) # this generates the logits for BCELoss\n",
    "        #x = self.sigmoid(self.out_layer(x, adj))\n",
    "        # TODO, implement test time!!!\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gnn_model, data):\n",
    "        # canonical training loop for a Pytorch Geometric GNN model gnn_model\n",
    "    # create dataloader for training\n",
    "    dataloader = DataLoader(dataset=data, batch_size = 2**7)\n",
    "    # define loss function\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    # define optimiser\n",
    "    optimizer = torch.optim.Adam(gnn_model.parameters(), lr = 1e-3)\n",
    "    # loop over 10 training epochs\n",
    "    for epoch in range(10):\n",
    "        # set model to training mode\n",
    "        gnn_model.train()\n",
    "        # loop over minibatches for training\n",
    "        for (k, batch) in enumerate(dataloader):\n",
    "            # compute current value of loss function via forward pass\n",
    "            output = gnn_model(batch)\n",
    "            loss_function_value = loss_function(output[:,0], torch.tensor(batch.y, dtype = torch.float32))\n",
    "            # set past gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "            # compute current gradient via backward pass\n",
    "            loss_function_value.backward()\n",
    "            # update model weights using gradient and optimisation method\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a16602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, indices):\n",
    "    '''\n",
    "    Helper function for creating a batch during training. Builds a batch \n",
    "    of source and target elements from the dataset. See the next cell for \n",
    "    when and how it's used. \n",
    "    \n",
    "    Arguments:\n",
    "        dataset: List[db_element] -- A list of dataset elements\n",
    "        indices: List[int] -- A list of indices of the dataset to sample\n",
    "    Returns:\n",
    "        batch_input: List[List[int]] -- List of tensorized names\n",
    "        batch_target: List[int] -- List of numerical categories\n",
    "        batch_indices: List[int] -- List of starting indices of padding\n",
    "    '''\n",
    "    # Recover what the entries for the batch are\n",
    "    batch = [dataset[i] for i in indices]\n",
    "    batch_input = np.array(list(zip(*batch))[0])\n",
    "    batch_target = np.array(list(zip(*batch))[1])\n",
    "    batch_indices = np.array(list(zip(*batch))[2])\n",
    "    return batch_input, batch_target, batch_indices # lines, categories\n",
    "\n",
    "def train(model, optimizer, criterion=nn.BCEWithLogitsLoss(), epochs, batch_size, seed):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        random.seed(seed + epoch)\n",
    "        np.random.seed(seed + epoch)\n",
    "        torch.manual_seed(seed + epoch)\n",
    "        indices = np.random.permutation(range(len(train_data)))\n",
    "        n_correct, n_total = 0, 0\n",
    "        progress_bar = tqdm(range(0, (len(train_data) // batch_size) + 1))\n",
    "        for i in progress_bar:\n",
    "            batch = build_batch(train_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "            (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "            (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "            logits = model(batch_input, batch_indices)\n",
    "            loss = criterion(logits, batch_target)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            n_correct += (predictions == batch_target).sum().item()\n",
    "            n_total += batch_target.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                progress_bar.set_description(f\"Epoch: {epoch}  Iteration: {i}  Loss: {np.mean(train_losses[-10:])}\")\n",
    "        train_accuracies.append(n_correct / n_total * 100)\n",
    "        print(f\"Epoch: {epoch}  Train Accuracy: {n_correct / n_total * 100}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            indices = list(range(len(test_data)))\n",
    "            n_correct, n_total = 0, 0\n",
    "            for i in range(0, (len(test_data) // batch_size) + 1):\n",
    "                batch = build_batch(test_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "                (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "                (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "                logits = model(batch_input, batch_indices)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                n_correct += (predictions == batch_target).sum().item()\n",
    "                n_total += batch_target.size(0)\n",
    "            eval_accuracies.append(n_correct / n_total * 100)\n",
    "            print(f\"Epoch: {epoch}  Eval Accuracy: {n_correct / n_total * 100}\")\n",
    "    \n",
    "    to_save = {\n",
    "        \"history\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"eval_accuracies\": eval_accuracies,\n",
    "        },\n",
    "        \"hparams\": {\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dropout\": dropout,\n",
    "            \"optimizer_class\": optimizer_class.__name__,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"seed\": seed\n",
    "        },\n",
    "        \"model\": [\n",
    "            (name, list(param.shape))\n",
    "            for name, param in rnn_model.named_parameters()\n",
    "        ]\n",
    "    }\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88380a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(model, optimizer, criterion=nn.BCEWithLogitsLoss(), epochs, batch_size, seed):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_accuracies = []\n",
    "    dataloader = DataLoader(dataset=data, batch_size = 2**7, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        random.seed(seed + epoch)\n",
    "        np.random.seed(seed + epoch)\n",
    "        torch.manual_seed(seed + epoch)\n",
    "        #indices = np.random.permutation(range(len(train_data)))\n",
    "        n_correct, n_total = 0, 0\n",
    "        #progress_bar = tqdm(range(0, (len(train_data) // batch_size) + 1))\n",
    "        progress_bar = tqdm(dataloader)\n",
    "        for i, (x, y) in enumerate(progress_bar):\n",
    "#             batch = build_batch(train_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "#             (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "#             (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "            logits = model(x, y)\n",
    "            loss = criterion(logits, y)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            n_correct += (predictions == y).sum().item()\n",
    "            n_total += y.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                progress_bar.set_description(f\"Epoch: {epoch}  Iteration: {i}  Loss: {np.mean(train_losses[-10:])}\")\n",
    "        train_accuracies.append(n_correct / n_total * 100)\n",
    "        print(f\"Epoch: {epoch}  Train Accuracy: {n_correct / n_total * 100}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            indices = list(range(len(test_data)))\n",
    "            n_correct, n_total = 0, 0\n",
    "            for i in range(0, (len(test_data) // batch_size) + 1):\n",
    "                batch = build_batch(test_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "                (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "                (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "                logits = model(batch_input, batch_indices)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                n_correct += (predictions == batch_target).sum().item()\n",
    "                n_total += batch_target.size(0)\n",
    "            eval_accuracies.append(n_correct / n_total * 100)\n",
    "            print(f\"Epoch: {epoch}  Eval Accuracy: {n_correct / n_total * 100}\")\n",
    "    \n",
    "    to_save = {\n",
    "        \"history\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"eval_accuracies\": eval_accuracies,\n",
    "        },\n",
    "        \"hparams\": {\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dropout\": dropout,\n",
    "            \"optimizer_class\": optimizer_class.__name__,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"seed\": seed\n",
    "        },\n",
    "        \"model\": [\n",
    "            (name, list(param.shape))\n",
    "            for name, param in rnn_model.named_parameters()\n",
    "        ]\n",
    "    }\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b932b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data...):\n",
    "    # Initilize model (ClassificationViT)\n",
    "    split_idx = int(0.8 * len(data))\n",
    "    trainloader = DataLoader(dataset=data[:split_idx], batch_size = 2**7, shuffle=True)\n",
    "    testloader = DataLoader(dataset=data[split_idx:], batch_size=2**7, shuffle=True)\n",
    "    model = DrugGCN(...)\n",
    "    # Move model to GPU \n",
    "    model.to(torch_device)\n",
    "    # Create optimizer for the model\n",
    "\n",
    "    # You may want to tune these hyperparameters to get better performance\n",
    "    #optimizer = optim.AdamW(model.parameters(), lr=1e-3, betas=(0.9, 0.95), weight_decay=1e-9)\n",
    "    optimizer = optim.Adam\n",
    "\n",
    "    total_steps = 0\n",
    "    num_epochs = 10\n",
    "    train_logfreq = 10 # maybe 100\n",
    "    losses = []\n",
    "    train_acc = []\n",
    "    all_val_acc = []\n",
    "    best_val_acc = 0\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    epoch_iterator = trange(num_epochs)\n",
    "    for epoch in epoch_iterator:\n",
    "        # Train\n",
    "        data_iterator = tqdm(trainloader)\n",
    "        for x, y in data_iterator:\n",
    "            total_steps += 1\n",
    "            x, y = x.to(torch_device), y.to(torch_device)\n",
    "            logits = model(x) # TODO: remember to pre-compute the adjacency matrix!!!\n",
    "            loss = loss_fn(logits, y)\n",
    "            accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            data_iterator.set_postfix(loss=loss.item(), train_acc=accuracy.item())\n",
    "\n",
    "            if total_steps % train_logfreq == 0:\n",
    "                losses.append(loss.item())\n",
    "                train_acc.append(accuracy.item())\n",
    "\n",
    "        # Validation\n",
    "        val_acc = []\n",
    "        model.eval()\n",
    "        for x, y in testloader:\n",
    "            x, y = x.to(torch_device), y.to(torch_device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(x)\n",
    "            accuracy = torch.mean((torch.argmax(logits, dim=-1) == y).float())\n",
    "            val_acc.append(accuracy.item())\n",
    "        model.train()\n",
    "\n",
    "        all_val_acc.append(np.mean(val_acc))\n",
    "        # Save best model\n",
    "        if np.mean(val_acc) > best_val_acc:\n",
    "            best_val_acc = np.mean(val_acc)\n",
    "\n",
    "        epoch_iterator.set_postfix(val_acc=np.mean(val_acc), best_val_acc=best_val_acc)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.title('Train Loss')\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc)\n",
    "    plt.title('Train Accuracy')\n",
    "    plt.figure()\n",
    "    plt.plot(all_val_acc)\n",
    "    plt.title('Val Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
